{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required Libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Document A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textA= open('DocA.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = textA.lower()\n",
    "# str.maketrans removes any punctuations \n",
    "cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Using word_tokenize to tokenize sentence into words\n",
    "tokenized_wordsA = word_tokenize(cleaned_text, \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW of DocA using countVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['100', '19655', '1988', '1989', '1990s', '1994', '1995', '1999', '2000s', '2010', '201013', '2010s', '2011', '2012', '2014', '2015', '2016', '2017', '2018', '27', '335', '377', '71', '82nd', 'aapke', 'abdul', 'according', 'achieved', 'acting', 'acting6', 'action', 'actor', 'actors', 'addition', 'after', 'aisi', 'also', 'amitabh', 'an', 'and', 'appeal', 'appeal15161718', 'april', 'arjun', 'as', 'aside', 'awards', 'bachchan', 'bail', 'began', 'being', 'bigg', 'biwi', 'blackbuck', 'bollywood', 'boss', 'both', 'brief', 'but', 'by', 'car', 'career', 'case', 'causes', 'celebrity', 'charity', 'cinema78', 'cited', 'comedy', 'commercially', 'continued', 'controversy', 'convicted', 'conviction', 'culpable', 'currently', 'dabangg', 'december', 'decline', 'drama', 'driving', 'earnings', 'ek', 'eldest', 'entertainers', 'family', 'film', 'filmfare', 'films', 'five', 'followed', 'for', 'forbes', 'foundation14', 'greater', 'hai', 'hain', 'has', 'he', 'heard21', 'help', 'highest', 'him', 'hindi', 'his', 'ho', 'homicide', 'host', 'hum', 'human', 'humanitarian', 'imprisonment1920', 'in', 'included', 'including', 'indian', 'info', 'is', 'karan', 'khan', 'khans', 'kick', 'killing', 'kiya', 'known', 'koun', 'lead', 'leading', 'legal', 'life', 'like', 'list', 'maine', 'marred', 'media', 'million1112', 'million910', 'most', 'national', 'negligent', 'no', 'no1', 'numerous', 'occasional', 'of', 'offscreen', 'on', 'one', 'out', 'over', 'people', 'period', 'personality', 'playing', 'poaching', 'presenter', 'producer', 'productions', 'promotes', 'pronounced', 'pronunciation', 'pyar', 'ran', 'rank', 'ranked', 'rashid', 'ready', 'reality', 'received', 'role', 'roles', 'romantic', 'saathsaath', 'salim', 'salman', 'screenwriter', 'sentenced', 'set', 'several', 'show', 'since', 'singer', 'slman', 'son', 'spanning', 'stardom', 'successful', 'sultan', 'supporting', 'television', 'tha', 'the', 'their', 'thirty', 'thriller', 'through', 'tied', 'tiger', 'to', 'toppaid', 'troubles', 'two', 'was', 'which', 'while', 'with', 'world', 'xan', 'years', 'zinda']\n"
     ]
    }
   ],
   "source": [
    "# This step will convert text into tokens \n",
    "vect1 = CountVectorizer()\n",
    "\n",
    "vect1.fit_transform(tokenized_wordsA)\n",
    "print(\"bag of words :\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abdul': 25,\n",
       " 'rashid': 165,\n",
       " 'salim': 173,\n",
       " 'salman': 174,\n",
       " 'khan': 119,\n",
       " 'pronounced': 159,\n",
       " 'slman': 182,\n",
       " 'xan': 207,\n",
       " 'hindi': 103,\n",
       " 'pronunciation': 160,\n",
       " 'help': 100,\n",
       " 'info': 116,\n",
       " '27': 19,\n",
       " 'december': 77,\n",
       " '19655': 1,\n",
       " 'is': 117,\n",
       " 'an': 38,\n",
       " 'indian': 115,\n",
       " 'film': 86,\n",
       " 'actor': 31,\n",
       " 'producer': 156,\n",
       " 'occasional': 143,\n",
       " 'singer': 181,\n",
       " 'and': 39,\n",
       " 'television': 189,\n",
       " 'personality': 152,\n",
       " 'in': 112,\n",
       " 'career': 61,\n",
       " 'spanning': 184,\n",
       " 'over': 149,\n",
       " 'thirty': 193,\n",
       " 'years': 208,\n",
       " 'has': 97,\n",
       " 'received': 168,\n",
       " 'numerous': 142,\n",
       " 'awards': 46,\n",
       " 'including': 114,\n",
       " 'two': 201,\n",
       " 'national': 138,\n",
       " 'as': 44,\n",
       " 'filmfare': 87,\n",
       " 'for': 91,\n",
       " 'acting6': 29,\n",
       " 'he': 98,\n",
       " 'cited': 67,\n",
       " 'the': 191,\n",
       " 'media': 134,\n",
       " 'one': 147,\n",
       " 'of': 144,\n",
       " 'most': 137,\n",
       " 'commercially': 69,\n",
       " 'successful': 186,\n",
       " 'actors': 32,\n",
       " 'both': 56,\n",
       " 'world': 206,\n",
       " 'cinema78': 66,\n",
       " 'forbes': 92,\n",
       " 'included': 113,\n",
       " 'him': 102,\n",
       " 'their': 192,\n",
       " '2015': 15,\n",
       " 'list': 131,\n",
       " 'toppaid': 199,\n",
       " '100': 0,\n",
       " 'celebrity': 64,\n",
       " 'entertainers': 84,\n",
       " 'tied': 196,\n",
       " 'with': 205,\n",
       " 'amitabh': 37,\n",
       " 'bachchan': 47,\n",
       " 'no': 140,\n",
       " '71': 22,\n",
       " 'on': 146,\n",
       " 'earnings': 81,\n",
       " '335': 20,\n",
       " 'million910': 136,\n",
       " 'according': 26,\n",
       " 'to': 198,\n",
       " '2018': 18,\n",
       " 'was': 202,\n",
       " 'highest': 101,\n",
       " 'ranked': 164,\n",
       " '82nd': 23,\n",
       " 'rank': 163,\n",
       " '377': 21,\n",
       " 'million1112': 135,\n",
       " 'also': 36,\n",
       " 'known': 124,\n",
       " 'host': 107,\n",
       " 'reality': 167,\n",
       " 'show': 179,\n",
       " 'bigg': 51,\n",
       " 'boss': 55,\n",
       " 'since': 180,\n",
       " '201013': 10,\n",
       " 'eldest': 83,\n",
       " 'son': 183,\n",
       " 'screenwriter': 175,\n",
       " 'began': 49,\n",
       " 'his': 104,\n",
       " 'acting': 28,\n",
       " 'supporting': 188,\n",
       " 'role': 169,\n",
       " 'biwi': 52,\n",
       " 'ho': 105,\n",
       " 'aisi': 35,\n",
       " '1988': 2,\n",
       " 'followed': 90,\n",
       " 'by': 59,\n",
       " 'leading': 127,\n",
       " 'maine': 132,\n",
       " 'pyar': 161,\n",
       " 'kiya': 123,\n",
       " '1989': 3,\n",
       " 'continued': 70,\n",
       " 'bollywood': 54,\n",
       " '1990s': 4,\n",
       " 'roles': 170,\n",
       " 'several': 178,\n",
       " 'productions': 157,\n",
       " 'romantic': 171,\n",
       " 'drama': 79,\n",
       " 'hum': 108,\n",
       " 'aapke': 24,\n",
       " 'hain': 96,\n",
       " 'koun': 125,\n",
       " '1994': 5,\n",
       " 'action': 30,\n",
       " 'thriller': 194,\n",
       " 'karan': 118,\n",
       " 'arjun': 43,\n",
       " '1995': 6,\n",
       " 'comedy': 68,\n",
       " 'no1': 141,\n",
       " '1999': 7,\n",
       " 'family': 85,\n",
       " 'saathsaath': 172,\n",
       " 'after': 34,\n",
       " 'brief': 57,\n",
       " 'period': 151,\n",
       " 'decline': 78,\n",
       " '2000s': 8,\n",
       " 'achieved': 27,\n",
       " 'greater': 94,\n",
       " 'stardom': 185,\n",
       " '2010s': 11,\n",
       " 'playing': 153,\n",
       " 'lead': 126,\n",
       " 'films': 88,\n",
       " 'like': 130,\n",
       " 'dabangg': 76,\n",
       " '2010': 9,\n",
       " 'ready': 166,\n",
       " '2011': 12,\n",
       " 'ek': 82,\n",
       " 'tha': 190,\n",
       " 'tiger': 197,\n",
       " '2012': 13,\n",
       " 'kick': 121,\n",
       " '2014': 14,\n",
       " 'sultan': 187,\n",
       " '2016': 16,\n",
       " 'zinda': 209,\n",
       " 'hai': 95,\n",
       " '2017': 17,\n",
       " 'addition': 33,\n",
       " 'presenter': 155,\n",
       " 'promotes': 158,\n",
       " 'humanitarian': 110,\n",
       " 'causes': 63,\n",
       " 'through': 195,\n",
       " 'charity': 65,\n",
       " 'being': 50,\n",
       " 'human': 109,\n",
       " 'foundation14': 93,\n",
       " 'khans': 120,\n",
       " 'offscreen': 145,\n",
       " 'life': 129,\n",
       " 'marred': 133,\n",
       " 'controversy': 71,\n",
       " 'legal': 128,\n",
       " 'troubles': 200,\n",
       " 'convicted': 72,\n",
       " 'culpable': 74,\n",
       " 'homicide': 106,\n",
       " 'negligent': 139,\n",
       " 'driving': 80,\n",
       " 'case': 62,\n",
       " 'which': 203,\n",
       " 'ran': 162,\n",
       " 'five': 89,\n",
       " 'people': 150,\n",
       " 'car': 60,\n",
       " 'killing': 122,\n",
       " 'but': 58,\n",
       " 'conviction': 73,\n",
       " 'set': 177,\n",
       " 'aside': 45,\n",
       " 'appeal15161718': 41,\n",
       " 'april': 42,\n",
       " 'blackbuck': 53,\n",
       " 'poaching': 154,\n",
       " 'sentenced': 176,\n",
       " 'imprisonment1920': 111,\n",
       " 'currently': 75,\n",
       " 'out': 148,\n",
       " 'bail': 48,\n",
       " 'while': 204,\n",
       " 'appeal': 40,\n",
       " 'heard21': 99}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textB= open('DocB.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = textB.lower()\n",
    "# str.maketrans removes any punctuations \n",
    "cleaned_textB = lower_case.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Using word_tokenize to tokenize sentence into words\n",
    "tokenized_wordsB = word_tokenize(cleaned_textB, \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW of DocB using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['14', '1965', '1980s', '1992', '1993', '1994', '1995', '1997', '1998', '1999', '2000', '2001', '2002', '2004', '2007', '2010', '2013', '2014', '2015', '2017', '80', 'acclaim', 'accolades', 'actor', 'after', 'alcoholic', 'also', 'an', 'and', 'anjaam', 'appearances', 'appeared', 'arts', 'arx', 'as', 'asia', 'asperger', 'audience', 'awarded', 'awards', 'baadshah', 'baazigar', 'been', 'began', 'bollywood', 'born', 'by', 'career', 'chak', 'chennai', 'coach', 'comedies', 'communities', 'connections', 'crime', 'critical', 'darr', 'de', 'debut', 'deewana', 'des', 'described', 'devdas', 'diaspora', 'differences', 'dil', 'dilwale', 'display', 'dulhania', 'early', 'earn', 'earned', 'et', 'express', 'film', 'filmfare', 'films', 'following', 'for', 'france', 'gender', 'gham', 'government', 'grievances', 'hai', 'happy', 'has', 'he', 'highestgrossing', 'him', 'his', 'hockey', 'honour', 'hota', 'identity', 'in', 'include', 'including', 'income', 'india', 'indian', 'initialism', 'is', 'jayenge', 'kabhi', 'kabhie', 'khan', 'khushi', 'king', 'known', 'kuch', 'late', 'le', 'legion', 'lettres', 'made', 'man', 'many', 'media', 'mohabbatein', 'more', 'most', 'my', 'name', 'nasa', 'national', 'new', 'november', 'numerous', 'of', 'on', 'one', 'or', 'ordre', 'padma', 'pagal', 'personality', 'portrayal', 'portraying', 'producer', 'prominence', 'pronounced', 'racial', 'raees', 'recognised', 'reference', 'referred', 'religious', 'roles', 'romantic', 'rose', 'rukh', 'scientist', 'series', 'several', 'shah', 'shri', 'significant', 'size', 'social', 'srk', 'starring', 'stars', 'successful', 'swades', 'syndrome', 'television', 'terms', 'than', 'the', 'themes', 'then', 'to', 'villainous', 'was', 'went', 'with', 'worlda', 'worldwide', 'xan', 'year']\n"
     ]
    }
   ],
   "source": [
    "# This step will convert text into tokens \n",
    "vect2 = CountVectorizer()\n",
    "\n",
    "vect2.fit_transform(tokenized_wordsB)\n",
    "print(\"bag of words :\",vect2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shah': 155,\n",
       " 'rukh': 151,\n",
       " 'khan': 106,\n",
       " 'pronounced': 141,\n",
       " 'arx': 33,\n",
       " 'xan': 179,\n",
       " 'born': 45,\n",
       " 'november': 127,\n",
       " '1965': 1,\n",
       " 'also': 26,\n",
       " 'known': 109,\n",
       " 'by': 46,\n",
       " 'the': 169,\n",
       " 'initialism': 101,\n",
       " 'srk': 160,\n",
       " 'is': 102,\n",
       " 'an': 27,\n",
       " 'indian': 100,\n",
       " 'actor': 23,\n",
       " 'film': 74,\n",
       " 'producer': 139,\n",
       " 'and': 28,\n",
       " 'television': 166,\n",
       " 'personality': 136,\n",
       " 'referred': 146,\n",
       " 'to': 172,\n",
       " 'in': 95,\n",
       " 'media': 118,\n",
       " 'as': 34,\n",
       " 'baadshah': 40,\n",
       " 'of': 129,\n",
       " 'bollywood': 44,\n",
       " 'reference': 145,\n",
       " 'his': 90,\n",
       " '1999': 9,\n",
       " 'king': 108,\n",
       " 'he': 87,\n",
       " 'has': 86,\n",
       " 'appeared': 31,\n",
       " 'more': 120,\n",
       " 'than': 168,\n",
       " '80': 20,\n",
       " 'films': 76,\n",
       " 'earned': 71,\n",
       " 'numerous': 128,\n",
       " 'accolades': 22,\n",
       " 'including': 97,\n",
       " '14': 0,\n",
       " 'filmfare': 75,\n",
       " 'awards': 39,\n",
       " 'government': 82,\n",
       " 'india': 99,\n",
       " 'awarded': 38,\n",
       " 'him': 89,\n",
       " 'padma': 134,\n",
       " 'shri': 156,\n",
       " 'france': 79,\n",
       " 'ordre': 133,\n",
       " 'des': 60,\n",
       " 'arts': 32,\n",
       " 'et': 72,\n",
       " 'lettres': 114,\n",
       " 'legion': 113,\n",
       " 'honour': 92,\n",
       " 'significant': 157,\n",
       " 'following': 77,\n",
       " 'asia': 35,\n",
       " 'diaspora': 63,\n",
       " 'worldwide': 178,\n",
       " 'terms': 167,\n",
       " 'audience': 37,\n",
       " 'size': 158,\n",
       " 'income': 98,\n",
       " 'been': 42,\n",
       " 'described': 61,\n",
       " 'one': 131,\n",
       " 'most': 121,\n",
       " 'successful': 163,\n",
       " 'stars': 162,\n",
       " 'worlda': 177,\n",
       " 'began': 43,\n",
       " 'career': 47,\n",
       " 'with': 176,\n",
       " 'appearances': 30,\n",
       " 'several': 154,\n",
       " 'series': 153,\n",
       " 'late': 111,\n",
       " '1980s': 2,\n",
       " 'made': 115,\n",
       " 'debut': 58,\n",
       " '1992': 3,\n",
       " 'deewana': 59,\n",
       " 'early': 69,\n",
       " 'was': 174,\n",
       " 'recognised': 144,\n",
       " 'for': 78,\n",
       " 'portraying': 138,\n",
       " 'villainous': 173,\n",
       " 'roles': 148,\n",
       " 'baazigar': 41,\n",
       " '1993': 4,\n",
       " 'darr': 56,\n",
       " 'anjaam': 29,\n",
       " '1994': 5,\n",
       " 'then': 171,\n",
       " 'rose': 150,\n",
       " 'prominence': 140,\n",
       " 'after': 24,\n",
       " 'starring': 161,\n",
       " 'romantic': 149,\n",
       " 'dilwale': 66,\n",
       " 'dulhania': 68,\n",
       " 'le': 112,\n",
       " 'jayenge': 103,\n",
       " '1995': 6,\n",
       " 'dil': 65,\n",
       " 'pagal': 135,\n",
       " 'hai': 84,\n",
       " '1997': 7,\n",
       " 'kuch': 110,\n",
       " 'hota': 93,\n",
       " '1998': 8,\n",
       " 'mohabbatein': 119,\n",
       " '2000': 10,\n",
       " 'kabhi': 104,\n",
       " 'khushi': 107,\n",
       " 'kabhie': 105,\n",
       " 'gham': 81,\n",
       " '2001': 11,\n",
       " 'went': 175,\n",
       " 'on': 130,\n",
       " 'earn': 70,\n",
       " 'critical': 55,\n",
       " 'acclaim': 21,\n",
       " 'portrayal': 137,\n",
       " 'alcoholic': 25,\n",
       " 'devdas': 62,\n",
       " '2002': 12,\n",
       " 'nasa': 124,\n",
       " 'scientist': 152,\n",
       " 'swades': 164,\n",
       " '2004': 13,\n",
       " 'hockey': 91,\n",
       " 'coach': 50,\n",
       " 'chak': 48,\n",
       " 'de': 57,\n",
       " '2007': 14,\n",
       " 'man': 116,\n",
       " 'asperger': 36,\n",
       " 'syndrome': 165,\n",
       " 'my': 122,\n",
       " 'name': 123,\n",
       " '2010': 15,\n",
       " 'highestgrossing': 88,\n",
       " 'include': 96,\n",
       " 'comedies': 51,\n",
       " 'chennai': 49,\n",
       " 'express': 73,\n",
       " '2013': 16,\n",
       " 'happy': 85,\n",
       " 'new': 126,\n",
       " 'year': 180,\n",
       " '2014': 17,\n",
       " '2015': 18,\n",
       " 'crime': 54,\n",
       " 'raees': 143,\n",
       " '2017': 19,\n",
       " 'many': 117,\n",
       " 'display': 67,\n",
       " 'themes': 170,\n",
       " 'national': 125,\n",
       " 'identity': 94,\n",
       " 'connections': 53,\n",
       " 'communities': 52,\n",
       " 'or': 132,\n",
       " 'gender': 80,\n",
       " 'racial': 142,\n",
       " 'social': 159,\n",
       " 'religious': 147,\n",
       " 'differences': 64,\n",
       " 'grievances': 83}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and transform and predict if the word is present or not\n",
    "#This is widely used for document or subject classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vect = CountVectorizer()\n",
    "c_vect.fit(tokenized_wordsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array \n",
      "  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "features names \n",
      " ['100', '19655', '1988', '1989', '1990s', '1994', '1995', '1999', '2000s', '2010', '201013', '2010s', '2011', '2012', '2014', '2015', '2016', '2017', '2018', '27', '335', '377', '71', '82nd', 'aapke', 'abdul', 'according', 'achieved', 'acting', 'acting6', 'action', 'actor', 'actors', 'addition', 'after', 'aisi', 'also', 'amitabh', 'an', 'and', 'appeal', 'appeal15161718', 'april', 'arjun', 'as', 'aside', 'awards', 'bachchan', 'bail', 'began', 'being', 'bigg', 'biwi', 'blackbuck', 'bollywood', 'boss', 'both', 'brief', 'but', 'by', 'car', 'career', 'case', 'causes', 'celebrity', 'charity', 'cinema78', 'cited', 'comedy', 'commercially', 'continued', 'controversy', 'convicted', 'conviction', 'culpable', 'currently', 'dabangg', 'december', 'decline', 'drama', 'driving', 'earnings', 'ek', 'eldest', 'entertainers', 'family', 'film', 'filmfare', 'films', 'five', 'followed', 'for', 'forbes', 'foundation14', 'greater', 'hai', 'hain', 'has', 'he', 'heard21', 'help', 'highest', 'him', 'hindi', 'his', 'ho', 'homicide', 'host', 'hum', 'human', 'humanitarian', 'imprisonment1920', 'in', 'included', 'including', 'indian', 'info', 'is', 'karan', 'khan', 'khans', 'kick', 'killing', 'kiya', 'known', 'koun', 'lead', 'leading', 'legal', 'life', 'like', 'list', 'maine', 'marred', 'media', 'million1112', 'million910', 'most', 'national', 'negligent', 'no', 'no1', 'numerous', 'occasional', 'of', 'offscreen', 'on', 'one', 'out', 'over', 'people', 'period', 'personality', 'playing', 'poaching', 'presenter', 'producer', 'productions', 'promotes', 'pronounced', 'pronunciation', 'pyar', 'ran', 'rank', 'ranked', 'rashid', 'ready', 'reality', 'received', 'role', 'roles', 'romantic', 'saathsaath', 'salim', 'salman', 'screenwriter', 'sentenced', 'set', 'several', 'show', 'since', 'singer', 'slman', 'son', 'spanning', 'stardom', 'successful', 'sultan', 'supporting', 'television', 'tha', 'the', 'their', 'thirty', 'thriller', 'through', 'tied', 'tiger', 'to', 'toppaid', 'troubles', 'two', 'was', 'which', 'while', 'with', 'world', 'xan', 'years', 'zinda']\n"
     ]
    }
   ],
   "source": [
    "c_new_vect = c_vect.transform(tokenized_wordsB)\n",
    "\n",
    "print (\"array \\n \",c_new_vect.toarray())\n",
    "\n",
    "# Compare with the indexes\n",
    "print (\"features names \\n\", vect1.get_feature_names() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TfidVectorizer for words present of DocB to DocA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# And now testing TFIDF vectorizer:\n",
    "# You can still specify n-grams here.\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(tokenized_wordsA)\n",
    "\n",
    "\n",
    "# Testing the TFIDF value + ngrams:\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [6.18178355]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing TFIDF vectorizer without normalization:\n",
    "# You can still specify n-grams here.\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2), norm=None)\n",
    "\n",
    "X = vectorizer.fit_transform(tokenized_wordsA)\n",
    "\n",
    "# Testing TFIDF value before normalization:\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
